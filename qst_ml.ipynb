{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Oihev5Cs2KBNHCbjB+zw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javahedi/QuantumStateTomography-ML/blob/main/qst_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrvkQ5tveNRH",
        "outputId": "b7d2e71f-67fe-47fe-8f27-fca282f5be9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path= \"/content/drive/MyDrive/SL_data/\"\n",
        "#name_list=os.listdir(path) # Makes a list with all file names\n",
        "cvs_names = glob.glob(f'{path}*.csv')\n",
        "\n",
        "data_list = []\n",
        "for id, name in enumerate(cvs_names):\n",
        "    #print(name)\n",
        "    data         = np.loadtxt( os.path.join(path,cvs_names[id]),delimiter=',',skiprows=1)\n",
        "\n",
        "\n",
        "    # scaled_x   between [0,1]\n",
        "    #data[:,0] = np.sin(data[:,0]) #/ (2 * np.pi)  # theta\n",
        "    #data[:,1] = np.sin(data[:,1]) #/ np.pi       # phi\n",
        "    data_list.append(data[:,:2])\n",
        "dataset = np.stack(data_list)\n",
        "print(dataset.shape)  # This will print the shape of the batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnrdQ4fKeRwN",
        "outputId": "8f9d0e48-0055-431c-aece-a4022ae11bc4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 99, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define your custom probability function\n",
        "def probability(y1, y2):\n",
        "    # Your custom logic here\n",
        "    p_out = y1 + y2  # Replace this with your actual logic\n",
        "    return p_out\n",
        "\n",
        "# Define the custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Assuming y_true and y_pred are of shape (batch_size, 2)\n",
        "    y1_true, y2_true = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
        "    y1_pred, y2_pred = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
        "\n",
        "    # Calculate the probability using the custom function\n",
        "    p_true = probability(y1_true, y2_true)\n",
        "    p_pred = probability(y1_pred, y2_pred)\n",
        "\n",
        "    # Calculate MSE loss\n",
        "    mse_loss = tf.keras.losses.mean_squared_error(p_true, p_pred)\n",
        "\n",
        "    return mse_loss\n",
        "\n",
        "\n",
        "# Stage 1: Feature Reduction\n",
        "class Stage1(tf.keras.Model):\n",
        "    def __init__(self, units, filters, kernel_size, lstm_units):\n",
        "        super(Stage1, self).__init__()\n",
        "\n",
        "        self.use_dense = layers.Dense(units, activation='relu')\n",
        "        self.use_cnn   = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                       activation='relu', padding='same')\n",
        "        self.use_lstm  = layers.LSTM(lstm_units, return_sequences=True)\n",
        "\n",
        "    def call(self, inputs, model_type, training=None, mask=None):\n",
        "        if model_type == 'dense':\n",
        "            output = self.use_dense(inputs)\n",
        "        elif model_type == 'cnn':\n",
        "            output = self.use_cnn(inputs)\n",
        "        elif model_type == 'lstm':\n",
        "            output = self.use_lstm(inputs)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model_type. Choose 'dense', 'cnn', or 'lstm'.\")\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1d = layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
        "                                    activation='relu', padding='same')\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        output = self.conv1d(inputs)\n",
        "        return output\n",
        "\n",
        "class LSTM(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = layers.LSTM(units, return_sequences=True)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        output = self.lstm(inputs)\n",
        "        return output\n",
        "\n",
        "class FiLM(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Feature-wise Linear Modulation (FiLM):\n",
        "    FiLM layers dynamically modulate the output of another layer based on the input.\n",
        "    This can be useful for conditional feature modulation:\n",
        "    \"\"\"\n",
        "    def __init__(self, units):\n",
        "        super(FiLM, self).__init__()\n",
        "        self.dense = layers.Dense(units, activation='relu')\n",
        "        self.film = layers.Dense(units * 2)  # Twice the units for gamma and beta\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.dense(inputs)\n",
        "        film_params = self.film(inputs)\n",
        "        gamma, beta = tf.split(film_params, 2, axis=-1)\n",
        "        output = gamma * x + beta\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class Stage1(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Stage1, self).__init__()\n",
        "        self.dense_layer = layers.Dense(1, activation='linear')  # Linear activation for regression\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        output = self.dense_layer(inputs)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Stage 2: Permutation Invariance\n",
        "class Stage2(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Stage2, self).__init__()\n",
        "\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "              num_heads=num_heads, key_dim=d_model // num_heads\n",
        "            )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        attention_output = self.attention(inputs, inputs)\n",
        "        pooled_output    = self.pooling(attention_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "# Stage 3: Dense Feed-Forward Network\n",
        "class Stage3(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Stage3, self).__init__()\n",
        "        self.dense_layer1 = layers.Dense(64, activation='relu')\n",
        "        self.dense_layer2 = layers.Dense(2, activation='linear')  # Two outputs with linear activation for regression\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x      = self.dense_layer1(inputs)\n",
        "        output = self.dense_layer2(x)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "mmwJ-YUv6g_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Connect the models\n",
        "num_samples     = 1500\n",
        "sequence_length = 50\n",
        "input_features  = 3\n",
        "batch_size      = 32\n",
        "num_epochs      = 10\n",
        "\n",
        "input_data      = tf.keras.Input(shape=(num_samples, sequence_length, input_features))\n",
        "\n",
        "# Stage 1\n",
        "model_type    = 'cnn'  # Change this to 'dense', 'cnn', or 'lstm'\n",
        "stage1_model  = Stage1(units=64, filters=32, kernel_size=3, lstm_units=64) # Create an instance of Stage1\n",
        "stage1_output = stage1_model(input_data, model_type=model_type)\n",
        "\n",
        "\n",
        "# Stage 2\n",
        "num_heads        = 8\n",
        "d_model          = 64\n",
        "stage2_attention = Stage2(num_heads, d_model)  # Create an instance of Stage2\n",
        "stage2_output    = stage2_attention(stage1_output)\n",
        "\n",
        "\n",
        "# Stage 3\n",
        "stage3_output = Stage3()(stage2_output)\n",
        "\n",
        "\n",
        "# Create the final model\n",
        "hybrid_model = tf.keras.Model(inputs=input_data, outputs=stage3_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "x9BS-yrFJSR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we have X_train and y_train\n",
        "# X_train has shape (num_samples, sequence_length, input_features)\n",
        "# y_train has shape (num_samples, 2)\n",
        "# Adjust the shape of X_train to (num_samples, num_features, num_components)\n",
        "X_train = np.random.random((int(num_samples*0.8), sequence_length, input_features))\n",
        "y_train = np.random.random((int(num_samples*0.8), 2))\n",
        "\n",
        "\n",
        "X_val = np.random.random((int(num_samples*0.2), sequence_length, input_features))\n",
        "y_val = np.random.random((int(num_samples*0.2), 2))"
      ],
      "metadata": {
        "id": "_gM8O1yefMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create callbacks\n",
        "checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath='model_checkpoint.h5',\n",
        "    monitor='val_loss',  # Choose the metric to monitor\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "early_stopping_callback = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Choose the metric to monitor\n",
        "    patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "hybrid_model.compile(optimizer='adam', loss=custom_loss)\n",
        "\n",
        "\n",
        "# Fit the model with batching and callbacks\n",
        "history = hybrid_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val),  # Optional: provide validation data for monitoring\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "21kSi3kYGE7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8MSkCGtC6g7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}