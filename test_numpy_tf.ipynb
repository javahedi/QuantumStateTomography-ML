{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg1tgNj3sqdeEFjUwEs7P9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javahedi/QuantumStateTomography-ML/blob/main/test_numpy_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "b4o2l3ShtvkZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um1Fr9c_t9Oz",
        "outputId": "9ca6c973-8cc1-4685-defc-69fe1c7ff0cd"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path         = \"/content/drive/MyDrive/SL_data/\"\n",
        "cvs_names    = glob.glob(f'{path}*.csv')\n",
        "bank_names   = glob.glob(f'{path}*bank.npy')\n",
        "weight_names = glob.glob(f'{path}*weights.npy')\n",
        "\n",
        "angles_list  = []\n",
        "banks_list   = []\n",
        "weights_list = []\n",
        "for id, name in enumerate(cvs_names):\n",
        "    data    = np.loadtxt( os.path.join(path,cvs_names[id]),delimiter=',',skiprows=1)\n",
        "    bank    = np.load(bank_names[id],mmap_mode=\"r\")\n",
        "    weights = np.load(weight_names[id],mmap_mode=\"r\")\n",
        "\n",
        "    angles_list.append(data)\n",
        "    banks_list.append(bank[1:])\n",
        "    weights_list.append(weights[1:])\n",
        "\n",
        "angles_data   = np.stack(angles_list)\n",
        "banks_data    = np.stack(banks_list)\n",
        "weights_data  = np.stack(weights_list)\n",
        "\n",
        "print(angles_data.shape)\n",
        "print(banks_data.shape)\n",
        "print(weights_data.shape)"
      ],
      "metadata": {
        "id": "UXwF80eHufPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889ccbab-28a6-40a7-b3b2-337faa860d85"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 99, 5)\n",
            "(1000, 99, 100, 2, 2)\n",
            "(1000, 99, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InfoGain:\n",
        "    def information_gain(self, angles, bank_particles, weights):\n",
        "        angle_dict = {\n",
        "            \"theta\": angles[0],\n",
        "            \"phi\": angles[1]\n",
        "        }\n",
        "        best_guess = np.array(np.einsum('i...,i', bank_particles, weights))\n",
        "        return self.adaptive_cost_func(angle_dict, bank_particles, weights, best_guess, 1)\n",
        "\n",
        "    def adaptive_cost_func(self, angles, rhoBank, weights, bestGuess, nQubits):\n",
        "        meshState = self.angles_to_state_vector(angles, nQubits)\n",
        "        out = np.einsum('ij,ik->ijk', meshState, meshState.conj())\n",
        "        K = self.Shannon_entropy(np.einsum('ijk,kj->i', out, bestGuess))\n",
        "        J = self.Shannon_entropy(np.einsum('ijk,lkj->il', out, rhoBank))\n",
        "        return np.real(K - np.dot(J, weights))\n",
        "\n",
        "    def Shannon_entropy(self, prob):\n",
        "        return np.real(np.sum(-(prob * np.log2(prob)), axis=0))\n",
        "\n",
        "    def angles_to_state_vector(self, angles, nQubits):\n",
        "        if nQubits == 1:\n",
        "            tempMesh = np.array([np.cos(angles[\"theta\"] / 2), np.exp(1j * angles[\"phi\"]) * np.sin(angles[\"theta\"] / 2)])\n",
        "            meshState = np.array([tempMesh, self.get_opposing_state(tempMesh)])\n",
        "        else:\n",
        "            tempMeshA = np.array([np.cos(angles[\"thetaA\"] / 2), np.exp(1j * angles[\"phiA\"]) * np.sin(angles[\"thetaA\"] / 2)])\n",
        "            tempMeshB = np.array([np.cos(angles[\"thetaB\"] / 2), np.exp(1j * angles[\"phiB\"]) * np.sin(angles[\"thetaB\"] / 2)])\n",
        "            meshA = np.array([tempMeshA, self.get_opposing_state(tempMeshA)])\n",
        "            meshB = np.array([tempMeshB, self.get_opposing_state(tempMeshB)])\n",
        "            meshState = np.array([\n",
        "                np.kron(meshA[0], meshB[0]),\n",
        "                np.kron(meshA[0], meshB[1]),\n",
        "                np.kron(meshA[1], meshB[0]),\n",
        "                np.kron(meshA[1], meshB[1])\n",
        "            ])\n",
        "        return meshState\n",
        "\n",
        "    def get_opposing_state(self, meshState):\n",
        "        if meshState[1] == 0:\n",
        "            return np.array([0, 1], dtype=complex)\n",
        "\n",
        "        a = 1\n",
        "        b = -np.conjugate(meshState[0]) / np.conjugate(meshState[1])\n",
        "        norm = np.sqrt(a * np.conjugate(a) + b * np.conjugate(b))\n",
        "        oppositeMeshState = np.array([a / norm, b / norm])\n",
        "        return oppositeMeshState\n",
        "\n",
        "info_gain = InfoGain()"
      ],
      "metadata": {
        "id": "o2T98nv2t3rc"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InfoGain_Vec:\n",
        "\n",
        "    def information_gain(self, angles,bank_particles,weights):\n",
        "        \"\"\"\n",
        "        Function is now vectorized in 0th index.\n",
        "        Takes in angles as [theta,phi] and bank_particles and weigt as how they are given by the file.\n",
        "        \"\"\"\n",
        "        best_guess=np.array(np.einsum('ijkl,ij->ikl',bank_particles,weights))\n",
        "        return self.adaptive_cost_func(angles,bank_particles,weights,best_guess)\n",
        "\n",
        "\n",
        "    def adaptive_cost_func(self, angles,rhoBank,weights,bestGuess):\n",
        "\n",
        "        # Crates projector from angles\n",
        "        povm=self.angles_to_single_qubit_POVM(angles)\n",
        "        # Computes the entropy of prior and posterior distributions. See 10.1103/PhysRevA.85.052120 for more details.\n",
        "        K=self.Shannon_entropy(np.einsum('nijk,nkj->ni',povm,bestGuess))\n",
        "        J=self.Shannon_entropy(np.einsum('nijk,nlkj->nil',povm,rhoBank))\n",
        "        # Returns the negative values such that it becomes a minimization problem rather than maximization problem.\n",
        "        return np.real(K-np.einsum(\"ij,ij->i\",J,weights))\n",
        "\n",
        "\n",
        "    def Shannon_entropy(self, prob):\n",
        "        \"\"\"\n",
        "        Function is now vectorized in 0th index.\n",
        "        Returns the shannon entorpy of the probability histogram.\n",
        "        \"\"\"\n",
        "        return np.real(np.sum(-(prob*np.log2(prob)),axis=1))\n",
        "\n",
        "    def angles_to_single_qubit_POVM(self, angles):\n",
        "        \"\"\"\n",
        "        Function is now vectorized in 0th index.\n",
        "        Takes in measurement angles as dictionaries and returns the spin POVM as 2x2x2 complex array .\n",
        "        For single qubit only.\n",
        "        \"\"\"\n",
        "        up_state_vector=np.array([np.cos(angles[:,0]/2),np.exp(1j*angles[:,1])*np.sin(angles[:,0]/2)],dtype=complex)\n",
        "        up_POVM=np.einsum(\"in,jn->nij\",up_state_vector,up_state_vector.conj())\n",
        "        return np.einsum('injk->nijk',np.array([up_POVM[:],np.eye(2)-up_POVM[:]],dtype=complex))\n",
        "\n",
        "info_gain_vec = InfoGain_Vec()"
      ],
      "metadata": {
        "id": "GFcMHAt1z4vD"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which step in ABME to predict\n",
        "prediction_index=50\n",
        "\n",
        "true_angle_np = angles_data[:,prediction_index,3:]\n",
        "banks_data_np = banks_data[:,prediction_index]\n",
        "weights_data_np = weights_data[:,prediction_index]\n",
        "true_info_gain_np=info_gain_vec.information_gain(true_angle_np,banks_data_np,weights_data_np)\n"
      ],
      "metadata": {
        "id": "j0FKZ7kbz4rB"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(true_angle_np.shape)\n",
        "print(banks_data_np.shape)\n",
        "print(weights_data_np.shape)\n",
        "print(true_info_gain_np.shape)"
      ],
      "metadata": {
        "id": "rda9I-rykoNu",
        "outputId": "f9a65589-f9c9-4bcb-8c40-93f2ebac139f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n",
            "(1000, 100, 2, 2)\n",
            "(1000, 100)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class InfoGain_Vec_TF(tf.Module):\n",
        "    def __init__(self):\n",
        "        super(InfoGain_Vec_TF, self).__init__()\n",
        "\n",
        "    def information_gain(self, angles, bank_particles, weights):\n",
        "        result = tf.numpy_function(self.information_gain_py, [angles, bank_particles, weights], tf.float64)\n",
        "        return tf.convert_to_tensor(result, dtype=tf.float64)\n",
        "\n",
        "    def information_gain_py(self, angles,bank_particles,weights):\n",
        "\n",
        "        best_guess=np.array(np.einsum('ijkl,ij->ikl',bank_particles,weights))\n",
        "        return self.adaptive_cost_func(angles,bank_particles,weights,best_guess)\n",
        "\n",
        "    def adaptive_cost_func(self, angles,rhoBank,weights,bestGuess):\n",
        "\n",
        "        # Crates projector from angles\n",
        "        povm=self.angles_to_single_qubit_POVM(angles)\n",
        "        # Computes the entropy of prior and posterior distributions. See 10.1103/PhysRevA.85.052120 for more details.\n",
        "        K=self.Shannon_entropy(np.einsum('nijk,nkj->ni',povm,bestGuess))\n",
        "        J=self.Shannon_entropy(np.einsum('nijk,nlkj->nil',povm,rhoBank))\n",
        "        # Returns the negative values such that it becomes a minimization problem rather than maximization problem.\n",
        "        return np.real(K-np.einsum(\"ij,ij->i\",J,weights))\n",
        "\n",
        "    #def Shannon_entropy(self, prob):\n",
        "    #    return np.real(np.sum(-(prob*np.log2(prob)),axis=1))\n",
        "\n",
        "    def Shannon_entropy(self, prob):\n",
        "        epsilon = 1e-10  # Small epsilon value to avoid division by zero\n",
        "        prob = np.maximum(prob, epsilon)  # Replace zeros with epsilon\n",
        "        return np.real(np.sum(-(prob * np.log2(prob)), axis=1))\n",
        "\n",
        "    def angles_to_single_qubit_POVM(self, angles):\n",
        "        \"\"\"\n",
        "        Function is now vectorized in 0th index.\n",
        "        Takes in measurement angles as dictionaries and returns the spin POVM as 2x2x2 complex array .\n",
        "        For single qubit only.\n",
        "        \"\"\"\n",
        "        up_state_vector=np.array([np.cos(angles[:,0]/2),np.exp(1j*angles[:,1])*np.sin(angles[:,0]/2)],dtype=complex)\n",
        "        up_POVM=np.einsum(\"in,jn->nij\",up_state_vector,up_state_vector.conj())\n",
        "        return np.einsum('injk->nijk',np.array([up_POVM[:],np.eye(2)-up_POVM[:]],dtype=complex))\n",
        "\n",
        "info_gain_vec_tf = InfoGain_Vec_TF()\n"
      ],
      "metadata": {
        "id": "hem670HwkoK0"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(true_angle_tf.shape)\n",
        "print(banks_data_tf.shape)\n",
        "print(weights_data_tf.shape)\n",
        "print(true_info_gain_tf.shape)"
      ],
      "metadata": {
        "id": "4vv-WaWCkoD9",
        "outputId": "4ed16aa4-ec23-4a9d-936c-a1a595513d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n",
            "(1000, 100, 2, 2)\n",
            "(1000, 100)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eB6uVM2hkn6z"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the percentage of data to use for validation\n",
        "validation_split = 0.2\n",
        "\n",
        "# Determine the number of samples to use for validation\n",
        "num_validation_samples = int(angles_data.shape[0] * validation_split)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data_angles  = angles_data[:-num_validation_samples]\n",
        "train_data_banks   = banks_data[:-num_validation_samples]\n",
        "train_data_weights = weights_data[:-num_validation_samples]\n",
        "\n",
        "validation_data_angles  = angles_data[-num_validation_samples:]\n",
        "validation_data_banks   = banks_data[-num_validation_samples:]\n",
        "validation_data_weights = weights_data[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "xmxdHdI5qUen"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size\n",
        "batch_size = 16\n",
        "\n",
        "# TensorFlow datasets API\n",
        "train_dataset      = tf.data.Dataset.from_tensor_slices((train_data_angles, train_data_banks, train_data_weights)).batch(batch_size)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_data_angles, validation_data_banks, validation_data_weights)).batch(batch_size)\n"
      ],
      "metadata": {
        "id": "qGKPs3LIqXlU"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimensions of the batches in training dataset\n",
        "for batch_angles, batch_banks, batch_weights in train_dataset.take(1):\n",
        "    print(\"Training Batch Shapes:\")\n",
        "    print(\"Angles Batch Shape:\" , batch_angles.shape)\n",
        "    print(\"Banks Batch Shape:\"  , batch_banks.shape)\n",
        "    print(\"Weights Batch Shape:\", batch_weights.shape)\n",
        "\n",
        "# Check dimensions of the batches in validation dataset\n",
        "for batch_angles, batch_banks, batch_weights in validation_dataset.take(1):\n",
        "    print(\"\\nValidation Batch Shapes:\")\n",
        "    print(\"Angles Batch Shape:\" , batch_angles.shape)\n",
        "    print(\"Banks Batch Shape:\"  , batch_banks.shape)\n",
        "    print(\"Weights Batch Shape:\", batch_weights.shape)"
      ],
      "metadata": {
        "id": "wBIQ0uMWqbxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46188f02-a088-44ee-d6ae-f24e699c6dca"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Batch Shapes:\n",
            "Angles Batch Shape: (16, 99, 5)\n",
            "Banks Batch Shape: (16, 99, 100, 2, 2)\n",
            "Weights Batch Shape: (16, 99, 100)\n",
            "\n",
            "Validation Batch Shapes:\n",
            "Angles Batch Shape: (16, 99, 5)\n",
            "Banks Batch Shape: (16, 99, 100, 2, 2)\n",
            "Weights Batch Shape: (16, 99, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_loss_function(y_true, y_pred, bank_particles, weights, lambda_weight=1.0):\n",
        "\n",
        "    info_gain_vec_tf = InfoGain_Vec_TF()\n",
        "\n",
        "    mse_target0 = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "    mse_target  = tf.reduce_mean(mse_target0, axis=1)\n",
        "\n",
        "    # average over samples\n",
        "    true_info_gains = 0\n",
        "    pred_info_gains = 0\n",
        "    for i in range(y_true.shape[1]):\n",
        "        true_info_gains += info_gain_vec_tf.information_gain(y_true[:,i,:], bank_particles[:,i,...], weights[:,i])\n",
        "        pred_info_gains += info_gain_vec_tf.information_gain(y_pred[:,i,:], bank_particles[:,i,...], weights[:,i])\n",
        "\n",
        "    true_info_gains = true_info_gains / y_true.shape[1]\n",
        "    pred_info_gains = pred_info_gains / y_true.shape[1]\n",
        "\n",
        "\n",
        "    loss_infoGain = 1.0 - (pred_info_gains / true_info_gains)\n",
        "\n",
        "    total_loss = mse_target + lambda_weight * tf.reduce_mean(loss_infoGain)\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "pUAZVwfj34Fc"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "CEL35oJhIRW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_angles, batch_banks, batch_weights in train_dataset.take(1):\n",
        "\n",
        "    y_true = batch_angles[...,:2]\n",
        "    y_pred = batch_angles[...,3:]\n",
        "\n",
        "    loss = custom_loss_function(y_true, y_pred, batch_banks, batch_weights)\n"
      ],
      "metadata": {
        "id": "b3xBPft64EBd"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69Wlsj_HtKZ",
        "outputId": "ffd6b1bf-bd2c-44bd-c9b9-c7c3b4b5f56b"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16,), dtype=float64, numpy=\n",
              "array([1.82469506, 2.64082368, 1.73837973, 1.94537335, 2.06721568,\n",
              "       2.13832443, 3.45353623, 2.2985041 , 3.16360485, 3.58051621,\n",
              "       2.5632466 , 3.27539746, 2.39465389, 1.16598685, 1.35482029,\n",
              "       2.21495136])>"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_function(y_true, y_pred, bank_particles, weights, lambda_weight=1.0):\n",
        "    info_gain_vec_tf = InfoGain_Vec_TF()\n",
        "\n",
        "    mse_target0 = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "    mse_target = tf.reduce_mean(mse_target0, axis=1)\n",
        "\n",
        "    def compute_info_gain(i):\n",
        "        true_info_gain = info_gain_vec_tf.information_gain(y_true[:, i, :], bank_particles[:, i, ...], weights[:, i])\n",
        "        pred_info_gain = info_gain_vec_tf.information_gain(y_pred[:, i, :], bank_particles[:, i, ...], weights[:, i])\n",
        "        return true_info_gain, pred_info_gain\n",
        "\n",
        "    true_info_gains, pred_info_gains = tf.map_fn(compute_info_gain, tf.range(y_true.shape[1]), dtype=(tf.float64, tf.float64))\n",
        "\n",
        "    true_info_gains = tf.reduce_mean(true_info_gains, axis=0)\n",
        "    pred_info_gains = tf.reduce_mean(pred_info_gains, axis=0)\n",
        "\n",
        "    loss_infoGain = 1.0 - (pred_info_gains / true_info_gains)\n",
        "\n",
        "    total_loss = mse_target + lambda_weight * tf.reduce_mean(loss_infoGain)\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "t8TKlNm4IUSV"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_angles, batch_banks, batch_weights in train_dataset.take(1):\n",
        "\n",
        "    y_true = batch_angles[...,:2]\n",
        "    y_pred = batch_angles[...,3:]\n",
        "\n",
        "    loss2 = custom_loss_function(y_true, y_pred, batch_banks, batch_weights)"
      ],
      "metadata": {
        "id": "78Pk8375IWKS"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss2\n"
      ],
      "metadata": {
        "id": "zP8ues88I7ld",
        "outputId": "5aae2b69-da9b-4006-aa58-aa77a3371853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16,), dtype=float64, numpy=\n",
              "array([1.82469506, 2.64082368, 1.73837973, 1.94537335, 2.06721568,\n",
              "       2.13832443, 3.45353623, 2.2985041 , 3.16360485, 3.58051621,\n",
              "       2.5632466 , 3.27539746, 2.39465389, 1.16598685, 1.35482029,\n",
              "       2.21495136])>"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    }
  ]
}